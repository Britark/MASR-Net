import math
from typing import Dict, List, Optional, Tuple
import warnings

import torch
import torch.nn.functional as F
from torch import Tensor, nn
from torch.nn import Parameter
from torch.cuda.amp import custom_fwd, custom_bwd, autocast

# æŠ‘åˆ¶custom_fwd/custom_bwdçš„FutureWarning
warnings.filterwarnings('ignore', category=FutureWarning, message='.*torch.cuda.amp.custom.*')

from fairseq import utils
from fairseq.incremental_decoding_utils import with_incremental_state
from fairseq.modules.fairseq_dropout import FairseqDropout
from fairseq.modules.quant_noise import quant_noise

"""
    æ­¤æ–‡ä»¶æ‰€æœ‰çš„patchå®é™…ä¸Šåœ¨ç½‘ç»œä¸­ä¸ºwindowï¼Œä½†ä¸ºäº†ç½‘ç»œçš„å³æ’å³ç”¨ä»ç„¶ç§°ä¹‹ä¸ºpatch
"""

@with_incremental_state
class MultiheadAttention(nn.Module):
    """Multi-headed attention.

    See "Attention Is All You Need" for more details.
    """

    def __init__(
            self,
            embed_dim,  # è¾“å…¥ç»´åº¦
            num_heads,  # top-kä¸ªæ³¨æ„åŠ›å¤´æ•°
            dropout=0.0,  # dropoutæ¦‚ç‡
            bias=False,  # çº¿æ€§å±‚åç½®
            q_noise=0.0,  # é‡åŒ–å™ªå£°å¼ºåº¦ï¼ˆç”¨äºæ¨¡å‹å‚æ•° é‡åŒ–ï¼Œé»˜è®¤0ä¸å¯ç”¨ï¼‰
            qn_block_size=8,  # é‡åŒ–å™ªå£°çš„åˆ†å—å¤§å°ï¼ˆä¸q_noiseé…åˆä½¿ç”¨ï¼‰
            num_expert=4,  # ä¸“å®¶æ•°é‡ï¼ˆç”¨äºç¨€ç–æ··åˆä¸“å®¶MoEç»“æ„ï¼‰
            head_dim=24,  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼ˆè‹¥è®¾ç½®ä¼šè¦†ç›–num_headsè®¡ç®—é€»è¾‘ï¼‰num_heads = embed_dim//head_dim
            use_attention_gate=False,  # æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›é—¨æ§ï¼Œé»˜è®¤ä¸èµ°
            cvloss=0,  # ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±ç³»æ•°ï¼ˆMoEä¸­å¹³è¡¡ä¸“å®¶ä½¿ç”¨çš„æŸå¤±é¡¹ï¼‰
            aux_loss=0,  # è¾…åŠ©æŸå¤±
            zloss=0,  # é—¨æ§è¾“å‡ºæ­£åˆ™åŒ–æŸå¤±ç³»æ•°ï¼ˆé˜²æ­¢é—¨æ§å€¼è¿‡å°ï¼‰
            sample_topk=0,  # æ¯ä¸ªæ ·æœ¬é€‰æ‹©topkä¸“å®¶è¿›è¡Œå‰å‘è®¡ç®—ï¼ˆ0è¡¨ç¤ºå…¨é€‰ï¼‰
            noisy_gating=False,  # æ˜¯å¦åœ¨é—¨æ§å‡½æ•°ä¸­åŠ å…¥å™ªå£°ï¼ˆå¢å¼ºMoEé²æ£’æ€§ï¼‰
            use_pos_bias=False,  # æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç çš„æ ‡å¿—ä½
    ):
        super().__init__()
        self.embed_dim = embed_dim  # æ¨¡å‹å®šä¹‰ç»´åº¦
        self.kdim = embed_dim  # Keyçš„ç»´åº¦ï¼Œç­‰äºembed_dim
        self.vdim = embed_dim  # åŒä¸Š

        self.num_heads = num_heads  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦embed_dim // num_heads
        # ä½¿ç”¨fairseqå®šåˆ¶çš„Dropoutæ¨¡å—
        self.dropout_module = FairseqDropout(
            dropout, module_name=self.__class__.__name__
        )

        self.head_dim = head_dim  # æ¯ä¸ªå¤´çš„ç»´åº¦ä¹Ÿå°±æ˜¯Qâ‹…Wq_iåçš„ç»´åº¦ä¹Ÿå°±æ˜¯è®¡ç®—æ³¨æ„åŠ›æœºåˆ¶çš„æ—¶å€™çš„ç»´åº¦
        self.scaling = self.head_dim ** -0.5  # ç¼©æ”¾å› å­1/âˆšd_k

        # linearKï¼Œè¿™ä¸ªå˜æ¢ä¹Ÿå°±æ˜¯KWkä¸­çš„Wkï¼Œä¸ªä¸“å®¶å…±äº«ç›¸åŒWk
        # q_noiseé‡åŒ–å™ªå£°æ¯”ä¾‹ (0.0~1.0)
        # qn_block_sizeå™ªå£°å—å¤§å° (å¦‚8è¡¨ç¤º8x8å—)

        # æ¯ä¸ªä¸“å®¶çš„linearQiï¼Œä¹Ÿå°±æ˜¯Wq_i
        self.q_proj = MoELinearWrapper(
            input_size=embed_dim,
            head_size=self.head_dim,
            num_experts=num_expert,
            k=self.num_heads,
            cvloss=cvloss,
            aux_loss=aux_loss,
            zloss=zloss,
            noisy_gating=noisy_gating
        )

        self.k_proj = quant_noise(
            nn.Linear(self.kdim, self.head_dim, bias=bias), q_noise, qn_block_size
        )
        # linearVï¼Œè¿™ä¸ªå˜æ¢ä¹Ÿå°±æ˜¯VWvä¸­çš„Wvï¼Œä¸ªä¸“å®¶å…±äº«ç›¸åŒWk
        self.v_proj = quant_noise(
            nn.Linear(self.vdim, self.head_dim, bias=bias), q_noise, qn_block_size
        )

        self.use_attention_gate = use_attention_gate

        self.sample_topk = sample_topk

        # ä½ç½®ç¼–ç ç›¸å…³å±æ€§
        self.use_pos_bias = use_pos_bias
        self.pos_bias = None  # ä½ç½®ç¼–ç åµŒå…¥è¡¨
        self.pos_indices = None  # ä½ç½®ç´¢å¼•

        if use_pos_bias:
            self.init_pos_bias()

        self.reset_parameters()  # åˆå§‹åŒ–æ¨¡å‹çš„æƒé‡

        self.skip_embed_dim_check = False  # æ˜¯å¦è·³è¿‡åµŒå…¥ç»´åº¦æ£€æŸ¥

        # å¯ç”¨CUDNNåŸºå‡†æµ‹è¯•ä»¥æé«˜æ€§èƒ½
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True

    def reset_parameters(self):
        # å½“Q/K/Vç»´åº¦ç›¸åŒæ—¶ï¼Œä½¿ç”¨ç¼©æ”¾ç‰ˆXavierå‡åŒ€åˆå§‹åŒ–k_proj(Wk)ï¼Œv_proj(Wv)æƒé‡ï¼ˆç»éªŒæ€§ä¼˜åŒ–ï¼‰
        # ç¼©æ”¾å› å­ 1/sqrt(2) ç”¨äºå¹³è¡¡å¤šå¤´æ³¨æ„åŠ›çš„åˆå¹¶ç»“æœ
        std_k = 1.0 / math.sqrt(self.kdim)
        std_v = 1.0 / math.sqrt(self.vdim)
        nn.init.normal_(self.k_proj.weight, 0, std_k)
        nn.init.normal_(self.v_proj.weight, 0, std_v)

        # nn.init.xavier_uniform_(self.out_proj.weight)
        # if self.out_proj.bias is not None:
        #     nn.init.constant_(self.out_proj.bias, 0.0)

    def init_pos_bias(self):
        """
        åˆå§‹åŒ–äºŒç»´ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œä¸“ä¸ºä¸­å¿ƒä½ç½®å¯¹å‘¨å›´8ä¸ªä½ç½®çš„æ³¨æ„åŠ›è®¾è®¡

        æ¯ä¸ªä½ç½®å­¦ä¹ ä¸€ä¸ªå‘é‡ï¼Œç»´åº¦ç­‰äºæ³¨æ„åŠ›å¤´çš„æ•°é‡
        """
        # åˆ›å»ºä½ç½®åç½®åµŒå…¥å±‚ï¼Œ8ä¸ªå›ºå®šçš„ç›¸å¯¹ä½ç½®
        self.pos_bias = nn.Embedding(8, self.num_heads)

        # ç›´æ¥ä½¿ç”¨ä»0åˆ°7çš„ç´¢å¼•è¡¨ç¤º8ä¸ªä½ç½®å…³ç³»
        self.pos_indices = torch.arange(8)

        # åªæœ‰å½“ç¼“å†²åŒºä¸å­˜åœ¨æ—¶æ‰æ³¨å†Œ
        if not hasattr(self, 'pos_indices'):
            self.register_buffer('pos_indices', self.pos_indices)

        # ä¸ºäº†æ¸…æ™°ï¼Œå¯ä»¥æ·»åŠ ä½ç½®åˆ°æ–¹å‘çš„æ˜ å°„æ³¨é‡Š
        # 0: å·¦ä¸Š, 1: ä¸Š, 2: å³ä¸Š, 3: å·¦, 4: å³, 5: å·¦ä¸‹, 6: ä¸‹, 7: å³ä¸‹

    def apply_pos_bias(self, attn_weights):
        """
        å°†ä½ç½®åç½®åº”ç”¨åˆ°æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µä¸Š

        å‚æ•°:
            attn_weights: æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µï¼Œå½¢çŠ¶ä¸º[bsz * patches, heads, tgt_len, src_len]

        è¿”å›:
            æ·»åŠ äº†ä½ç½®åç½®çš„æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
        """
        # ç¡®ä¿pos_indicesåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = attn_weights.device
        self.pos_indices = self.pos_indices.to(device)

        # ä»åµŒå…¥å±‚è·å–ä½ç½®åç½®
        bias = self.pos_bias(self.pos_indices)

        # æ­¥éª¤1: è°ƒæ•´ç»´åº¦é¡ºåºï¼Œä½¿headsç»´åº¦åœ¨å‰
        bias = bias.permute(1, 0)

        # æ­¥éª¤2: å°†æ¯ä¸ªä½ç½®ç¼–ç é‡å¤4æ¬¡ï¼Œç¡®ä¿æ¯è¿ç»­4ä¸ªä½ç½®å…±äº«åŒä¸€ç¼–ç 
        bias = bias.repeat_interleave(3, dim=1)

        # æ­¥éª¤3: æ·»åŠ ç»´åº¦ï¼Œä¸ºæ‰¹æ¬¡å’Œåºåˆ—é•¿åº¦åšå‡†å¤‡
        bias = bias.unsqueeze(0).unsqueeze(2)

        # æ­¥éª¤4: å°†biaså¹¿æ’­åˆ°ä¸attn_weightsç›¸åŒçš„ç»´åº¦
        bias_broadcasted = bias.expand_as(attn_weights)

        # æ­¥éª¤5: æ·»åŠ åˆ°æ³¨æ„åŠ›åˆ†æ•°ä¸Š
        return attn_weights + (bias_broadcasted / self.scaling)

    def forward(
            self,
            query,
            key: Optional[Tensor],  # å¯é€‰
            value: Optional[Tensor],  # å¯é€‰
            k_isp: Optional[Tensor] = None,
            need_weights: bool = True,  # æ˜¯å¦è¿”å›å¤šå¤´å¹³å‡æ³¨æ„åŠ›åˆ†æ•°
            before_softmax: bool = False,  # æ˜¯å¦è¿”å›softmaxå‰çš„åŸå§‹åˆ†æ•°
            need_head_weights: bool = False,  # æ˜¯å¦è¿”å›æ¯ä¸ªå¤´çš„ç‹¬ç«‹æ³¨æ„åŠ›åˆ†æ•°
    ) -> Tuple[Tensor, Optional[Tensor]]:
        """

        Args:
            need_weights (bool, å¯é€‰):
                æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡ï¼ˆç»è¿‡å¤šå¤´å¹³å‡åçš„ç»“æœï¼Œé»˜è®¤ä¸ºFalseï¼‰ã€‚
            before_softmax (bool, å¯é€‰):
                æ˜¯å¦è¿”å›ç»è¿‡softmaxå‰çš„åŸå§‹æ³¨æ„åŠ›æƒé‡å’Œå€¼ã€‚
            need_head_weights (bool, å¯é€‰):
                æ˜¯å¦è¿”å›æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„æƒé‡ï¼ˆä¼šå¼ºåˆ¶å¯ç”¨need_weightsï¼‰ã€‚
                é»˜è®¤è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å¤´çš„å¹³å‡æƒé‡ã€‚
        """
        # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®¡ç®—
        with autocast(enabled=torch.cuda.is_available()):
            # æ˜¯å¦è¿”å›æ¯ä¸ªå¤´çš„ç‹¬ç«‹æ³¨æ„åŠ›åˆ†æ•°
            if need_head_weights:
                need_weights = True

            bsz, patches, tgt_len, embed_dim = query.size()  # è¿”å›queryå¤§å°ï¼ˆç›®æ ‡åºåˆ—é•¿åº¦ï¼ˆISPè¶…å‚æ•°ï¼‰ã€æ‰¹æ¬¡(å›¾ç‰‡æ•°)ã€å—æ•°ã€åµŒå…¥ç»´åº¦ï¼‰
            src_len = tgt_len  # é»˜è®¤æºåºåˆ—é•¿åº¦ç­‰äºç›®æ ‡åºåˆ—é•¿åº¦ï¼ˆè‡ªæ³¨æ„åŠ›åœºæ™¯ï¼ŒMoA2ï¼‰
            # æ£€æŸ¥queryçš„åµŒå…¥ç»´åº¦æ˜¯å¦ç¬¦åˆé¢„æœŸï¼ˆä»…åœ¨å¯ç”¨æ£€æŸ¥æ—¶ï¼‰
            if not self.skip_embed_dim_check:
                assert (
                        embed_dim == self.embed_dim
                ), f"query dim {embed_dim} != {self.embed_dim}"  # queryè¾“å…¥ç»´åº¦ä¸ç­‰äºæ¨¡å‹å®šä¹‰ç»´åº¦
            assert list(query.size()) == [bsz, patches, tgt_len, embed_dim]  # æ£€æŸ¥queryçš„å°ºå¯¸æ˜¯å¦ç¬¦åˆæ ‡å‡†
            # å¤„ç†keyçš„å­˜åœ¨æƒ…å†µï¼ˆéè‡ªæ³¨æ„åŠ›æ—¶ä¼ å…¥ï¼‰
            if key is not None:
                key_bsz, key_p, src_len, _ = key.size()
                if not torch.jit.is_scripting():  # è„šæœ¬æ¨¡å¼å¤–è¿›è¡Œç»´åº¦éªŒè¯ï¼ˆTorchScriptä¼šè·³è¿‡è¿™äº›æ£€æŸ¥ï¼‰
                    assert key_bsz == bsz  # keyå’Œqueryçš„æ‰¹å¤§å°å¿…é¡»ç›¸åŒ
                    assert key_p == patches
                    assert value is not None  # ç”±keyå¿…é¡»è¦æœ‰value
                    assert (key_bsz, key_p, src_len) == value.shape[:3]  # keyå’Œvalueçš„å‰ä¸¤ä¸ªå°ºåº¦å¿…é¡»ä¸€æ ·

            # å¦‚æœæ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè®¡ç®—q(qWq_i), k(KWk), v(VWv)ï¼Œè¾…åŠ©æŸå¤±
            # qkvéƒ½ç”±queryç”Ÿæˆ
            """
                qçš„ç»“æ„
                [
                    [æ ·æœ¬1çš„ä¸“å®¶1æ˜ å°„, æ ·æœ¬1çš„ä¸“å®¶3æ˜ å°„],  # æ ·æœ¬1
                    [æ ·æœ¬2çš„ä¸“å®¶2æ˜ å°„, æ ·æœ¬2çš„ä¸“å®¶3æ˜ å°„],  # æ ·æœ¬2
                    [æ ·æœ¬3çš„ä¸“å®¶1æ˜ å°„, æ ·æœ¬3çš„ä¸“å®¶2æ˜ å°„],  # æ ·æœ¬3
                ]
            """

            assert key is not None and value is not None
            # ç¡®ä¿å¼ é‡è¿ç»­ä»¥ä¼˜åŒ–è®¡ç®—
            query_cont = query.contiguous()
            key_cont = key.contiguous()
            value_cont = value.contiguous()

            q, aux_loss = self.q_proj.map(
                query_cont, k_isp=k_isp, sample_topk=self.sample_topk,
                attention_gate=self.use_attention_gate
            )  # q å½¢çŠ¶ä¸º[batch_size, p, tgt_len, k, head_size]
            k = self.k_proj(key_cont)  # k çš„å½¢çŠ¶ä¸º [bsz, p, src_len, head_dim]
            v = self.v_proj(value_cont)  # v çš„å½¢çŠ¶ä¸º [bsz, p, src_len, head_dim]
            q *= self.scaling  # qä¹˜ç¼©æ”¾å› å­1/âˆšd_kï¼Œä¹‹åç›´æ¥å’Œkç‚¹ç§¯å°±è¡Œï¼Œsoftmax((q/âˆšd_k)k)

            assert k is not None
            assert k.size(2) == src_len

            '''
            è®¡ç®—è‡ªæ³¨æ„åŠ›åˆ†æ•°
            qçš„å½¢çŠ¶æ˜¯(b,p,i,k,e)ï¼šæ‰¹æ¬¡(å›¾ç‰‡æ•°)ã€å—æ•°ã€ç›®æ ‡åºåˆ—é•¿åº¦ï¼ˆISPè¶…å‚æ•°ï¼‰ã€æ³¨æ„åŠ›å¤´æ•°é‡ã€æ¯ä¸ªå¤´çš„åµŒå…¥ç»´åº¦
            kçš„å½¢çŠ¶æ˜¯(b,p,j,e)ï¼šæ‰¹æ¬¡(å›¾ç‰‡æ•°)ã€å—æ•°ã€æºåºåˆ—é•¿åº¦ï¼ˆæ¯å—çš„tokenæ•°ï¼‰ã€åµŒå…¥ç»´åº¦
            'bpike,bpje->bpkij'è¾“å‡ºå¼ é‡çš„ç»´åº¦ï¼šbpkijï¼Œæ‰¹æ¬¡(å›¾ç‰‡æ•°)ã€å—æ•°ã€æ³¨æ„åŠ›å¤´æ•°é‡ã€ç›®æ ‡åºåˆ—é•¿åº¦ã€æºåºåˆ—é•¿åº¦
            '''
            attn_weights = torch.einsum('bpike,bpje->bpkij', q, k)

            # åº”ç”¨ä½ç½®ç¼–ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_pos_bias:
                # è·å–å½“å‰å½¢çŠ¶
                bsz, patches, num_heads, tgt_len, src_len = attn_weights.shape  # [bsz, patches, self.num_heads, tgt_len, src_len]

                # å°†attn_weightsé‡æ–°æ’åˆ—ä¸ºå½¢çŠ¶[bsz*patches, heads, tgt_len, src_len]
                attn_weights = attn_weights.reshape(bsz * patches, self.num_heads, tgt_len, src_len)

                # åº”ç”¨ä½ç½®ç¼–ç 
                attn_weights = self.apply_pos_bias(attn_weights)

                # æ¢å¤åŸå§‹å½¢çŠ¶ [bsz, patches, self.num_heads, tgt_len, src_len]
                attn_weights = attn_weights.reshape(bsz, patches, self.num_heads, tgt_len, src_len)

            bsz, patches, num_heads, tgt_len, src_len = attn_weights.shape

            attn_weights = attn_weights.contiguous().reshape(bsz * patches * self.num_heads, tgt_len,
                                                             src_len)  # å°†ç»“æœå¼ é‡é‡å¡‘ä¸ºç»´åº¦(æ‰¹æ¬¡å¤§å°Ã—å—æ•°Ã—æ³¨æ„åŠ›å¤´æ•°é‡ï¼Œç›®æ ‡é•¿åº¦ï¼Œæºé•¿åº¦)

            assert list(attn_weights.size()) == [bsz * patches * self.num_heads, tgt_len, src_len]

            # å¦‚æœéœ€è¦è¿”å›softmaxå‰çš„åŸå§‹åˆ†æ•°ï¼Œè¿”å›qk/âˆšd_kã€å’Œv
            if before_softmax:
                return attn_weights, v

            # å¯¹æ³¨æ„åŠ›æƒé‡åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šåº”ç”¨softmaxå‡½æ•°ï¼Œä½¿æƒé‡å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            attn_weights_float = utils.softmax(attn_weights, dim=-1)

            attn_weights = attn_weights_float.type_as(attn_weights)  # å°†æµ®ç‚¹å‹çš„æ³¨æ„åŠ›æƒé‡è½¬æ¢å›åŸå§‹æ³¨æ„åŠ›æƒé‡çš„æ•°æ®ç±»å‹
            # å¯¹æ³¨æ„åŠ›æƒé‡åº”ç”¨dropoutï¼Œéšæœºä¸¢å¼ƒå¼ é‡å…ƒç´ ï¼Œå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œdropoutä¼šè¢«å…³é—­ï¼Œæ‰€æœ‰æƒé‡éƒ½ä¼šè¢«ä¿ç•™ã€‚
            attn_probs = self.dropout_module(attn_weights)

            # softmax(qk/âˆšd_k)Â·v
            assert v is not None
            # é‡å¡‘ä¸ºåŸå§‹å½¢çŠ¶ä»¥ä¾¿einsumæ“ä½œ
            attn_probs_reshaped = attn_probs.reshape(bsz, patches, self.num_heads, tgt_len, src_len)
            attn = torch.einsum('bpkij,bpje->bpike', attn_probs_reshaped,
                                v)  # [bsz, patches, tgt_len, self.num_heads, self.head_dim]
            # æ³¨æ„åŠ›è®¡ç®—åçš„å½¢çŠ¶
            assert list(attn.size()) == [bsz, patches, tgt_len, self.num_heads, self.head_dim]

            attn = self.q_proj.reduce(
                attn)  # åŠ æƒæ±‚å’Œå¾—åˆ°MoAè¾“å‡º åŸä»£ç æ­¤å¤„ä¸ºattn = self.q_proj.reduce(attn).transpose(0, 1)ï¼Œå½¢çŠ¶æœ€ç»ˆä¸º[bsz, patches, tgt_len, self.head_dim]

            # é‡å¡‘æ³¨æ„åŠ›æƒé‡ä¸º[æ‰¹æ¬¡å¤§å°, æ³¨æ„åŠ›å¤´æ•°é‡, ç›®æ ‡é•¿åº¦, æºé•¿åº¦]å¹¶è½¬ç½®
            if need_weights:
                attn_weights = attn_weights_float.reshape(
                    bsz, patches, self.num_heads, tgt_len, src_len
                ).permute(2, 0, 1, 3, 4)  # [self.num_heads, bsz, patches, tgt_len, src_len]
                if not need_head_weights:
                    # average attention weights over heads
                    attn_weights = attn_weights.mean(dim=0)  # æ²¿ç€å¤´ç»´åº¦å¯¹æ³¨æ„åŠ›æƒé‡æ±‚å¹³å‡
            else:
                attn_weights: Optional[Tensor] = None

        return attn, attn_weights, aux_loss  # è¿”å›MoAæ³¨æ„åŠ›æœ€ç»ˆè®¡ç®—ç»“æœã€æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€è¾…åŠ©æŸå¤±


class MoE(nn.Module):
    """ ä½¿ç”¨ä¸€å±‚FFNä½œä¸ºä¸“å®¶çš„ç¨€ç–é—¨æ§ã€‚
        å‚æ•°:
        input_size: æ•´æ•° - è¾“å…¥çš„å¤§å°
        output_size: æ•´æ•° - è¾“å‡ºçš„å¤§å°
        num_experts: æ•´æ•° - ä¸“å®¶çš„æ•°é‡
        hidden_size: æ•´æ•° - ä¸“å®¶çš„éšè—å±‚å¤§å°
        noisy_gating: å¸ƒå°”å€¼ - æ˜¯å¦ä½¿ç”¨å™ªå£°é—¨æ§
        k: æ•´æ•° - å¯¹æ¯ä¸ªæ‰¹æ¬¡å…ƒç´ ä½¿ç”¨å¤šå°‘ä¸ªä¸“å®¶
    """

    def __init__(self, input_size, head_size, num_experts, k, need_merge=False, cvloss=0, aux_loss=0, zloss=0,
                 bias=False, activation=None, noisy_gating=True, hidden_sizes=None):
        super(MoE, self).__init__()
        self.noisy_gating = noisy_gating  # æ˜¯å¦ä½¿ç”¨å™ªå£°é—¨æ§
        self.num_experts = num_experts  # ä¸“å®¶æ€»æ•°é‡
        self.input_size = input_size  # è¾“å…¥ç»´åº¦
        self.head_size = head_size  # æ³¨æ„åŠ›å¤´å¤§å°
        self.need_merge = need_merge
        self.saved_top_k_indices = None
        self.token_expert_indices = None  # ç”¨äºå­˜å‚¨æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶ç´¢å¼•
        
        # æ”¯æŒå¤šå±‚FFNçš„ä¸“å®¶
        self.experts = ParallelExperts(num_experts, input_size, head_size, bias, hidden_sizes)  # å¹¶è¡Œä¸“å®¶å±‚ï¼Œæ”¯æŒå¤šå±‚FFN
        self.output_experts = ParallelExperts(num_experts, head_size, input_size, bias)  # è¾“å‡ºå±‚ä¿æŒå•å±‚
        
        self.k = min(k, self.num_experts)
        self.cvloss = cvloss  # å˜å¼‚ç³»æ•°æŸå¤±
        self.aux_loss = aux_loss  # åˆ‡æ¢æŸå¤±
        self.zloss = zloss  # Z_loss
        self.activation = activation
        # é—¨æ§æƒé‡çŸ©é˜µ
        # é—¨æ§æƒé‡çŸ©é˜µ
        # âœ… æ›´å¼ºçš„åˆå§‹åŒ–
        self.w_atten_gate = nn.Parameter(torch.randn(num_experts, num_experts) * 0.2, requires_grad=True)
        self.w_gate = nn.Parameter(torch.randn(input_size, num_experts) * 0.2, requires_grad=True)

        # ä½¿ç”¨LeCun Normalæ›¿ä»£Xavier
        std_gate = 1.0 / math.sqrt(input_size)
        std_atten = 1.0 / math.sqrt(num_experts)
        nn.init.normal_(self.w_gate, 0, std_gate)
        nn.init.normal_(self.w_atten_gate, 0, std_atten)

        if noisy_gating:
            self.w_noise = nn.Parameter(torch.randn(input_size, num_experts) * 0.01, requires_grad=True)

    def cv_squared(self, x):
        """ æ ·æœ¬çš„å˜å¼‚ç³»æ•°å¹³æ–¹ã€‚
            ä½œä¸ºæŸå¤±å‡½æ•°æ—¶ï¼Œé¼“åŠ±æ­£åˆ†å¸ƒæ›´åŠ å‡åŒ€ã€‚
            æ·»åŠ epsilonä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ã€‚
            å¯¹äºç©ºå¼ é‡è¿”å›0ã€‚
            Args:
                x: ä¸€ä¸ªå¼ é‡
            Returns:
                ä¸€ä¸ªæ ‡é‡
        """
        eps = 1e-10
        # if only num_experts = 1

        if x.shape[0] == 1:
            return 0
        return x.float().var() / (x.float().mean() ** 2 + eps)  # è®¡ç®—å˜å¼‚ç³»æ•°çš„å¹³æ–¹: æ–¹å·®é™¤ä»¥å‡å€¼çš„å¹³æ–¹

    # è®¡ç®—å˜å¼‚ç³»æ•°
    def compute_cvloss(self, probs):
        return self.cv_squared(F.normalize(probs.sum(0), p=1, dim=0))

    # 1. å¯¹è¾“å…¥çš„æ¦‚ç‡å¼ é‡ probs æŒ‰ç…§ç»´åº¦0æ±‚å’Œï¼Œå¾—åˆ°å„ä¸“å®¶çš„ç´¯ç§¯æ¦‚ç‡ã€‚
    # 2. ä½¿ç”¨ F.normalize è¿›è¡Œå½’ä¸€åŒ–æ“ä½œï¼Œå°†æ±‚å’Œåçš„ç»“æœè½¬æ¢ä¸ºå’Œä¸º1çš„åˆ†å¸ƒï¼Œå½’ä¸€åŒ–æ—¶ä½¿ç”¨ L1 èŒƒæ•°ï¼ˆp=1ï¼‰æ²¿ dim=0 è¿›è¡Œå½’ä¸€åŒ–ã€‚
    # 3. è°ƒç”¨ cv_squared å‡½æ•°è®¡ç®—å½’ä¸€åŒ–åæ¦‚ç‡åˆ†å¸ƒçš„ç³»æ•°å¹³æ–¹ã€‚

    def auxiliary_loss(self, probs, freqs):
        """ä¿®æ”¹ä¸ºæ”¯æŒæµ®ç‚¹freqså‚æ•°ï¼Œä¿æŒæ¢¯åº¦æµ"""
        # ç¡®ä¿éƒ½æ˜¯æµ®ç‚¹æ•°è¿›è¡Œè®¡ç®—ï¼Œé¿å…ç±»å‹è½¬æ¢æˆªæ–­æ¢¯åº¦
        probs_sum = F.normalize(probs.sum(0), p=1, dim=0)

        # â˜… å…³é”®æ–°å¢ä¸€è¡Œï¼šæ˜¾å¼è½¬ dtypeï¼Œé¿å… F.normalize() æŠ¥é”™
        freqs = freqs.to(probs.dtype)  # æˆ– freqs = freqs.float()

        freqs_norm = F.normalize(freqs, p=1, dim=0)
        loss = probs_sum * freqs_norm
        return loss.sum() * self.num_experts

    # 1. å¯¹è¾“å…¥çš„logits è¿›è¡ŒæŒ‡æ•°è¿ç®—ï¼Œè½¬æ¢ä¸ºæœªå½’ä¸€åŒ–çš„æ¦‚ç‡é‡åº¦ã€‚å¹¶æ²¿ç€ dim=1 æ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„å½’ä¸€åŒ–å› å­
    # 2. å¯¹æ±‚å’Œçš„ç»“æœå–å¯¹æ•°ï¼Œå†å¹³æ–¹å¹³æ–¹ã€‚
    # 3. è®¡ç®—ä¸Šè¿°å€¼çš„å‡å€¼ï¼Œä½œä¸º z loss çš„æœ€ç»ˆå€¼ã€‚
    def compute_zloss(self, logits):
        # é€šè¿‡ä½¿ç”¨log_softmaxä¼˜åŒ–æ•°å€¼ç¨³å®šæ€§
        log_sum = torch.logsumexp(logits, dim=1)
        return torch.mean(log_sum ** 2)

    def atten_gating(self, Q_isp, K_isp, sample_topk=1, noise_epsilon=1e-3):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            Q_isp (Tensor): å½¢çŠ¶ä¸º [batches, patches, 1, input_dim] çš„å¼ é‡
            K_isp (Tensor): å½¢çŠ¶ä¸º [batches, patches, N, input_dim] çš„å¼ é‡
            k (int): top-kä¸­çš„kå€¼ï¼Œè¡¨ç¤ºä¿ç•™çš„æƒé‡æ•°é‡

        è¿”å›:
            gate_weights (Tensor): å½¢çŠ¶ä¸º [-1, N] çš„å¼ é‡ï¼Œ
                                  è¡¨ç¤ºé—¨æ§æƒé‡ï¼Œæ¯ä¸ªå‘é‡ä¸­åªæœ‰kä¸ªéé›¶å…ƒç´ 
        """
        # ä½¿ç”¨æ··åˆç²¾åº¦è®¡ç®—
        with autocast(enabled=torch.cuda.is_available()):
            # è·å–è¾“å…¥å½¢çŠ¶
            batches, patches, N, input_dim = K_isp.shape
            scale = 1.0 / math.sqrt(input_dim)
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQ_ispä¸K_ispçš„ç‚¹ç§¯
            # Q_ispçš„å½¢çŠ¶æ˜¯[batches, patches, 1, input_dim]
            # K_ispçš„å½¢çŠ¶æ˜¯[batches, patches, N, input_dim]
            # ç¡®ä¿è¾“å…¥å¼ é‡æ˜¯è¿ç»­çš„
            Q_isp = Q_isp.contiguous()
            K_isp = K_isp.contiguous()

            attention_scores = torch.matmul(Q_isp, K_isp.transpose(-1, -2))  # [batches, patches, 1, N]

            # ç§»é™¤å¤šä½™çš„ç»´åº¦
            attention_scores = attention_scores.squeeze(2)  # [batches, patches, N]

            attention_scores = attention_scores.reshape(-1, N)  # [-1, N]

            attention_scores = attention_scores * scale

            attention_scores = torch.nan_to_num(
                attention_scores,
                nan=0.0,
                posinf=20.0,  # ä¸Šé™å¯æ ¹æ®éœ€è¦è°ƒåˆ°30ã€40
                neginf=-20.0
            )

            clean_logits = attention_scores
            # å¦‚æœå¯ç”¨äº†å™ªå£°é—¨æ§å¹¶ä¸”å½“å‰å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œåˆ™æ·»åŠ å™ªå£°
            if self.noisy_gating and self.training:
                # è®¡ç®—è¾“å…¥ x ä¸å™ªå£°æƒé‡çŸ©é˜µ self.w_noise çš„çŸ©é˜µä¹˜ç§¯ï¼Œå¾—åˆ°åŸå§‹å™ªå£°æ ‡å‡†å·®
                raw_noise_stddev = attention_scores @ self.w_atten_gate
                # é€šè¿‡ softplus æ¿€æ´»å‡½æ•°ä¿è¯å™ªå£°æ ‡å‡†å·®ä¸ºæ­£å€¼ï¼Œå¹¶åŠ ä¸Šä¸€ä¸ªæå°å€¼ noise_epsilon
                noise_stddev = torch.nan_to_num(
                    F.softplus(raw_noise_stddev) + noise_epsilon,
                    nan=noise_epsilon,
                    posinf=1e3, neginf=noise_epsilon
                )
                # ç”Ÿæˆä¸ clean_logits ç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°å¹¶æŒ‰å™ªå£°æ ‡å‡†å·®è¿›è¡Œç¼©æ”¾ï¼Œç„¶åä¸ clean_logits ç›¸åŠ 
                noisy_logits = clean_logits + (torch.randn_like(clean_logits) * noise_stddev)
                # ä½¿ç”¨å¸¦å™ªå£°çš„ logits
                logits = noisy_logits
            else:
                logits = clean_logits
            # åº”ç”¨softmaxè·å–æ³¨æ„åŠ›æƒé‡

            logits = torch.nan_to_num(
                logits,
                nan=0.0,
                posinf=20.0,
                neginf=-20.0
            )

            attention_weights = F.softmax(logits, dim=1)  # [-1, N]

            # å±•å¹³attention_weightså¹¶ä¿å­˜ä¸ºprobs
            probs = attention_weights

            # å¦‚æœè®­ç»ƒé˜¶æ®µï¼Œä¸”sample_topk > 0å¯ç”¨æ··åˆé€‰æ‹©ç­–ç•¥
            if sample_topk > 0:
                # top_k_indices = torch.multinomial(probs + 1e-6, self.k)
                # top_k_gates = torch.gather(probs, 1, top_k_indices)
                assert sample_topk <= self.k
                # å…ˆåœ¨æ‰€æœ‰ä¸“å®¶ä¸­é€‰æ‹©topkä¸­çš„topk - sample_topkä¸ª
                _, top_km1_indices = probs.topk(self.k - sample_topk, dim=1)
                masked_probs = probs + 1e-6  # æ·»åŠ æå°å€¼é¿å…é›¶æ¦‚ç‡
                masked_probs[torch.arange(probs.size(0)).unsqueeze(
                    1), top_km1_indices] = 0  # åˆ›å»ºæ©ç æ¦‚ç‡ï¼šå·²é€‰ä½ç½®ç½®é›¶é˜²æ­¢é‡å¤é€‰æ‹©
                k_indices = torch.multinomial(masked_probs, sample_topk)  # ä»å‰©ä½™æ¦‚ç‡ä¸­é‡‡æ · sample_topk ä¸ªä¸“å®¶
                # åˆå¹¶ç¡®å®šæ€§å’Œé‡‡æ ·å¾—åˆ°çš„ä¸“å®¶ç´¢å¼•
                top_k_indices = torch.cat([top_km1_indices, k_indices], dim=-1)
                top_k_gates = torch.gather(probs, 1, top_k_indices)
            else:
                # âœ… ä¼˜åŒ–ï¼šsorted=Falseé¿å…ä¸å¿…è¦çš„æ’åºï¼ŒåŠ é€Ÿé—¨æ§è®¡ç®—
                top_k_gates, top_k_indices = probs.topk(self.k, dim=1, sorted=False)  # å¸¸è§„top-ké€‰æ‹©ï¼ˆç¡®å®šæ€§é€‰æ‹©ï¼‰

            # é—¨æ§å€¼å½’ä¸€åŒ–
            top_k_gates = top_k_gates / \
                          (top_k_gates.sum(dim=1, keepdim=True) + 1e-6)

            # åˆ›å»ºå…¨é›¶å¼ é‡ç”¨äºå­˜å‚¨é—¨æ§æƒé‡
            zeros = torch.zeros_like(probs)  # âœ… ç§»é™¤requires_gradï¼Œè®©autogradè‡ªåŠ¨å¤„ç†
            zeros = zeros.requires_grad_(probs.requires_grad)  # âœ… æ˜ç¡®è®¾ç½®
            gates = zeros.scatter(1, top_k_indices, top_k_gates)

            # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ ·æœ¬æ•°ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡è®¡ç®—ï¼‰
            freqs_float = gates.sum(0)  # ç”¨æ¦‚ç‡å’Œ
            self.expert_size = (gates > 0).sum(0).long()  # åªæ˜¯ä¸ºäº† debug/æ‰“å°ï¼Œå¯ä¿ç•™
            # é—¨æ§å€¼å’Œå¯¹åº”ç´¢å¼•å±•å¹³ä¸º[batch_size * k]
            top_k_gates = top_k_gates.flatten()  # é—¨æ§å€¼[0.5, 0.3, 0.7, 0.9, 0.6, 0.8] 3 ä¸ªæ ·æœ¬ Ã— 2 ä¸ªä¸“å®¶
            top_k_experts = top_k_indices.flatten()  # é—¨æ§ç´¢å¼•[1, 3, 2, 4, 1, 2]3 ä¸ªæ ·æœ¬ Ã— 2 ä¸ªä¸“å®¶

            # ä»¥ä¸‹ä¸‰è¡Œæ˜¯å…³é”®ä»£ç ï¼Œä¸èƒ½ä¿®æ”¹ï¼Œä¿æŒåŸå§‹å®ç°é€»è¾‘
            # è¿‡æ»¤é›¶å€¼é—¨æ§ï¼ˆä¿ç•™æœ‰æ•ˆè·¯ç”±ä¿¡æ¯ï¼‰
            nonzeros = top_k_gates.nonzero().squeeze(-1)
            top_k_experts_nonzero = top_k_experts[nonzeros]  # é—¨æ§ç´¢å¼•[1, 3, 2, 4, 1, 2]
            # topkä¸ªä¸“å®¶æŒ‰IDæ’åºï¼Œä¼˜åŒ–åç»­è®¡ç®—æ•ˆç‡
            _, _index_sorted_experts = top_k_experts_nonzero.sort(0)  # æ’åºåçš„é—¨æ§ç´¢å¼•
            self.index_sorted_experts = nonzeros[_index_sorted_experts]

            """
            top_k_indices = [[1, 3],  # ç¬¬ä¸€ä¸ªæ ·æœ¬é€‰æ‹©äº†ä¸“å®¶ 1 å’Œ 3
                            [2, 4],  # ç¬¬äºŒä¸ªæ ·æœ¬é€‰æ‹©äº†ä¸“å®¶ 2 å’Œ 4
                            [1, 2]]  # ç¬¬ä¸‰ä¸ªæ ·æœ¬é€‰æ‹©äº†ä¸“å®¶ 1 å’Œ 2
            top_k_gates = [[0.5, 0.3],  # ç¬¬ä¸€ä¸ªæ ·æœ¬é€‰æ‹©ä¸“å®¶ 1 å’Œ 3 çš„é—¨æ§å€¼åˆ†åˆ«æ˜¯ 0.5 å’Œ 0.3
                        [0.7, 0.9],  # ç¬¬äºŒä¸ªæ ·æœ¬é€‰æ‹©ä¸“å®¶ 2 å’Œ 4 çš„é—¨æ§å€¼åˆ†åˆ«æ˜¯ 0.7 å’Œ 0.9
                        [0.6, 0.8]]  # ç¬¬ä¸‰ä¸ªæ ·æœ¬é€‰æ‹©ä¸“å®¶ 1 å’Œ 2 çš„é—¨æ§å€¼åˆ†åˆ«æ˜¯ 0.6 å’Œ 0.8
            """
            self.batch_index = self.index_sorted_experts.div(self.k,
                                                             rounding_mode='trunc')  # æ¯ä¸ªä¸“å®¶çš„æ ·æœ¬æ± ï¼ˆæ¯ä¸ªä¸“å®¶å¤„ç†å“ªäº›æ ·æœ¬ï¼Ÿæ ·æœ¬æ± æ˜¯æ ·æœ¬åŸå§‹ç´¢å¼•ç»„æˆçš„ï¼‰ï¼Œå†æŠŠæ ·æœ¬æ± æŒ‰ç…§ä¸“å®¶ç´¢å¼•æ’åºï¼Œ[ä¸“å®¶1æ ·æœ¬æ± ï¼Œä¸“å®¶2æ ·æœ¬æ± ï¼Œ...]
            self.batch_gates = top_k_gates[
                self.index_sorted_experts]  # æ¯ä¸ªä¸“å®¶çš„æ ·æœ¬é—¨æ§å€¼æ± ï¼ˆæ¯ä¸ªä¸“å®¶å¤„ç†çš„æ ·æœ¬å¯¹åº”çš„é—¨æ§å€¼ï¼‰ï¼Œå†æŠŠæ ·æœ¬é—¨æ§å€¼æ± æŒ‰ç…§ä¸“å®¶ç´¢å¼•æ’åºï¼Œ[ä¸“å®¶1æ ·æœ¬é—¨æ§å€¼æ± ï¼Œä¸“å®¶2æ ·æœ¬é—¨æ§å€¼æ± ï¼Œ...]
            # è®¡ç®—æŸå¤±
            loss = 0
            # å˜å¼‚ç³»æ•°æŸå¤±ï¼šé¼“åŠ±å‡åŒ€ä½¿ç”¨å„ä¸“å®¶
            loss += self.cvloss * self.compute_cvloss(gates)
            # è¾…åŠ©æŸå¤±
            loss += self.aux_loss * self.auxiliary_loss(probs, freqs_float)
            # zloss
            loss += self.zloss * self.compute_zloss(logits)

            return loss

    def top_k_gating(self, x, sample_topk=1, noise_epsilon=1e-3):
        """
            å™ªå£° top-k é—¨æ§ã€‚
            å‚è§è®ºæ–‡ï¼šhttps://arxiv.org/abs/1701.06538

            å‚æ•°:
              x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [-1, input_size]
              sample_topk: è®­ç»ƒé˜¶æ®µé‡‡æ ·é€‰å–çš„ä¸“å®¶æ•°é‡ï¼ˆ0 è¡¨ç¤ºä¸é‡‡æ ·ï¼‰
              noise_epsilon: é˜²æ­¢æ•°å€¼ä¸ç¨³å®šçš„å°æµ®ç‚¹æ•°
            è¿”å›:
              loss: ç»¼åˆäº† cv lossã€switch loss åŠ z loss çš„æœ€ç»ˆæŸå¤±å€¼
        """
        # ä½¿ç”¨æ··åˆç²¾åº¦è®¡ç®—
        with autocast(enabled=torch.cuda.is_available()):
            # ç¡®ä¿è¾“å…¥å¼ é‡è¿ç»­ä»¥ä¼˜åŒ–è®¡ç®—
            x = x.contiguous()

            d = x.size(1)
            scale = 1.0 / math.sqrt(d)
            # è®¡ç®—è¾“å…¥ x ä¸é—¨æ§æƒé‡çŸ©é˜µ self.w_gate çš„çŸ©é˜µä¹˜ç§¯ï¼Œå¾—åˆ°å¹²å‡€çš„ logits
            clean_logits = x @ self.w_gate
            clean_logits = clean_logits * scale
            clean_logits = torch.clamp(clean_logits, min=-10.0, max=10.0)
            # ===== DEBUG: æŸ¥çœ‹ clamp å‰å logits åˆ†å¸ƒ =====

            # å¦‚æœå¯ç”¨äº†å™ªå£°é—¨æ§å¹¶ä¸”å½“å‰å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œåˆ™æ·»åŠ å™ªå£°
            if self.noisy_gating and self.training:
                # è®¡ç®—è¾“å…¥ x ä¸å™ªå£°æƒé‡çŸ©é˜µ self.w_noise çš„çŸ©é˜µä¹˜ç§¯ï¼Œå¾—åˆ°åŸå§‹å™ªå£°æ ‡å‡†å·®
                raw_noise_stddev = x @ self.w_noise
                # é€šè¿‡ softplus æ¿€æ´»å‡½æ•°ä¿è¯å™ªå£°æ ‡å‡†å·®ä¸ºæ­£å€¼ï¼Œå¹¶åŠ ä¸Šä¸€ä¸ªæå°å€¼ noise_epsilon
                noise_stddev = torch.nan_to_num(
                    F.softplus(raw_noise_stddev) + noise_epsilon,
                    nan=noise_epsilon,
                    posinf=1e3, neginf=noise_epsilon
                )
                # ç”Ÿæˆä¸ clean_logits ç›¸åŒå½¢çŠ¶çš„éšæœºå™ªå£°å¹¶æŒ‰å™ªå£°æ ‡å‡†å·®è¿›è¡Œç¼©æ”¾ï¼Œç„¶åä¸ clean_logits ç›¸åŠ 
                noisy_logits = clean_logits + (torch.randn_like(clean_logits) * noise_stddev)
                # ä½¿ç”¨å¸¦å™ªå£°çš„ logits
                logits = noisy_logits
            else:
                logits = clean_logits

            logits = torch.nan_to_num(
                logits,
                nan=0.0,
                posinf=20.0,
                neginf=-20.0
            )

            probs = F.softmax(logits, dim=1)  # [-1, N]
            # âœ… è°ƒè¯•ä½ç½®2: æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ

            # å¦‚æœè®­ç»ƒé˜¶æ®µï¼Œä¸”sample_topk > 0å¯ç”¨æ··åˆé€‰æ‹©ç­–ç•¥
            if sample_topk > 0:
                # top_k_indices = torch.multinomial(probs + 1e-6, self.k)
                # top_k_gates = torch.gather(probs, 1, top_k_indices)
                assert sample_topk <= self.k
                # å…ˆåœ¨æ‰€æœ‰ä¸“å®¶ä¸­é€‰æ‹©topkä¸­çš„topk - sample_topkä¸ª
                _, top_km1_indices = probs.topk(self.k - sample_topk, dim=1)
                masked_probs = probs + 1e-6  # æ·»åŠ æå°å€¼é¿å…é›¶æ¦‚ç‡
                masked_probs[torch.arange(probs.size(0)).unsqueeze(
                    1), top_km1_indices] = 0  # åˆ›å»ºæ©ç æ¦‚ç‡ï¼šå·²é€‰ä½ç½®ç½®é›¶é˜²æ­¢é‡å¤é€‰æ‹©
                k_indices = torch.multinomial(masked_probs, sample_topk)  # ä»å‰©ä½™æ¦‚ç‡ä¸­é‡‡æ · sample_topk ä¸ªä¸“å®¶
                # åˆå¹¶ç¡®å®šæ€§å’Œé‡‡æ ·å¾—åˆ°çš„ä¸“å®¶ç´¢å¼•
                top_k_indices = torch.cat([top_km1_indices, k_indices], dim=-1)
                top_k_gates = torch.gather(probs, 1, top_k_indices)
            else:
                # âœ… ä¼˜åŒ–ï¼šsorted=Falseé¿å…ä¸å¿…è¦çš„æ’åºï¼ŒåŠ é€Ÿé—¨æ§è®¡ç®—
                top_k_gates, top_k_indices = probs.topk(self.k, dim=1, sorted=False)  # å¸¸è§„top-ké€‰æ‹©ï¼ˆç¡®å®šæ€§é€‰æ‹©ï¼‰
            # é—¨æ§å€¼å½’ä¸€åŒ–
            top_k_gates = top_k_gates / \
                          (top_k_gates.sum(dim=1, keepdim=True) + 1e-6)

            # ä¿å­˜ä¸ºå®ä¾‹å˜é‡ï¼Œä»¥ä¾¿åœ¨forwardå’Œconcatä¸­ä½¿ç”¨
            self.saved_top_k_indices = top_k_indices

            gates = torch.zeros_like(probs)  # åˆ›å»ºä¸ probs åŒå½¢çš„é›¶å¼ é‡
            gates.scatter_(1, top_k_indices, top_k_gates)  # å…³é”®ä¿®æ”¹ï¼šin-place æ“ä½œä¿ç•™è®¡ç®—å›¾

            # ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ ·æœ¬æ•°ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡è®¡ç®—ï¼‰
            expert_size_int = (gates > 0).sum(0).long()  # int64, å’Œ input è¡Œæ•°ä¸¥æ ¼å¯¹é½
            self.expert_size = expert_size_int  # ParallelLinear ä¼šç”¨åˆ°

            # 2) æµ®ç‚¹è®¡æ•° â€”â€” é—¨æ§æ¦‚ç‡ä¹‹å’Œï¼Œç”¨äºè´Ÿè½½å‡è¡¡æŸå¤±ï¼Œèƒ½ä¼ æ’­æ¢¯åº¦
            freqs_float = gates.sum(0)  # float32

            # é—¨æ§å€¼å’Œå¯¹åº”ç´¢å¼•å±•å¹³ä¸º[batch_size * k]
            top_k_gates = top_k_gates.flatten()  # é—¨æ§å€¼[0.5, 0.3, 0.7, 0.9, 0.6, 0.8] 3 ä¸ªæ ·æœ¬ Ã— 2 ä¸ªä¸“å®¶
            top_k_experts = top_k_indices.flatten()  # é—¨æ§ç´¢å¼•[1, 3, 2, 4, 1, 2]3 ä¸ªæ ·æœ¬ Ã— 2 ä¸ªä¸“å®¶

            # ä»¥ä¸‹ä¸‰è¡Œæ˜¯å…³é”®ä»£ç ï¼Œä¸èƒ½ä¿®æ”¹ï¼Œä¿æŒåŸå§‹å®ç°é€»è¾‘
            # è¿‡æ»¤é›¶å€¼é—¨æ§ï¼ˆä¿ç•™æœ‰æ•ˆè·¯ç”±ä¿¡æ¯ï¼‰
            nonzeros = top_k_gates.nonzero().squeeze(-1)
            top_k_experts_nonzero = top_k_experts[nonzeros]  # é—¨æ§ç´¢å¼•[1, 3, 2, 4, 1, 2]
            # topkä¸ªä¸“å®¶æŒ‰IDæ’åºï¼Œä¼˜åŒ–åç»­è®¡ç®—æ•ˆç‡
            _, _index_sorted_experts = top_k_experts_nonzero.sort(0)  # æ’åºåçš„é—¨æ§ç´¢å¼•
            self.index_sorted_experts = nonzeros[_index_sorted_experts]

            self.batch_index = self.index_sorted_experts.div(self.k,
                                                             rounding_mode='trunc')  # æ¯ä¸ªä¸“å®¶çš„æ ·æœ¬æ± ï¼ˆæ¯ä¸ªä¸“å®¶å¤„ç†å“ªäº›æ ·æœ¬ï¼Ÿæ ·æœ¬æ± æ˜¯æ ·æœ¬åŸå§‹ç´¢å¼•ç»„æˆçš„ï¼‰ï¼Œå†æŠŠæ ·æœ¬æ± æŒ‰ç…§ä¸“å®¶ç´¢å¼•æ’åºï¼Œ[ä¸“å®¶1æ ·æœ¬æ± ï¼Œä¸“å®¶2æ ·æœ¬æ± ï¼Œ...]
            self.batch_gates = top_k_gates[
                self.index_sorted_experts]  # æ¯ä¸ªä¸“å®¶çš„æ ·æœ¬é—¨æ§å€¼æ± ï¼ˆæ¯ä¸ªä¸“å®¶å¤„ç†çš„æ ·æœ¬å¯¹åº”çš„é—¨æ§å€¼ï¼‰ï¼Œå†æŠŠæ ·æœ¬é—¨æ§å€¼æ± æŒ‰ç…§ä¸“å®¶ç´¢å¼•æ’åºï¼Œ[ä¸“å®¶1æ ·æœ¬é—¨æ§å€¼æ± ï¼Œä¸“å®¶2æ ·æœ¬é—¨æ§å€¼æ± ï¼Œ...]
            # è®¡ç®—æŸå¤±
            loss = 0
            # å˜å¼‚ç³»æ•°æŸå¤±ï¼šé¼“åŠ±å‡åŒ€ä½¿ç”¨å„ä¸“å®¶
            loss += self.cvloss * self.compute_cvloss(gates)
            # è¾…åŠ©æŸå¤±
            loss += self.aux_loss * self.auxiliary_loss(probs, freqs_float)
            # zloss
            loss += self.zloss * self.compute_zloss(logits)
            # åœ¨å‡½æ•°æœ«å°¾æ·»åŠ 

            return loss

    """
        MoEå±‚å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œå®ç°åŠ¨æ€è·¯ç”±å’Œä¸“å®¶è®¡ç®—

        å‚æ•°:
          x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [bsz, patches, tgt_len, self.num_heads, self.head_dim]
          sample_topk: è®­ç»ƒæ—¶é‡‡æ ·çš„ä¸“å®¶æ•°é‡ï¼ˆ0è¡¨ç¤ºç¦ç”¨é‡‡æ ·ï¼Œæ­£å¸¸topké€‰æ‹©ï¼‰
          multiply_by_gates: æ˜¯å¦ç”¨é—¨æ§å€¼åŠ æƒä¸“å®¶è¾“å‡º

        è¿”å›:
          y: ç»è¿‡ä¸“å®¶èšåˆçš„è¾“å‡ºï¼Œå½¢çŠ¶åŒè¾“å…¥x
          loss: é—¨æ§æœºåˆ¶è®¡ç®—çš„æ€»æŸå¤±
    """

    def forward(self, x, sample_topk=1, multiply_by_gates=True):
        """
        å‰å‘ä¼ æ’­è¾“å…¥æ˜¯[bsz, patches, tgt_len, self.input_size]
        """
        # ä½¿ç”¨æ··åˆç²¾åº¦è®¡ç®—
        with autocast(enabled=torch.cuda.is_available()):
            # è¾“å…¥æ•°æ®é¢„å¤„ç†
            bsz, patches, length, emb_size = x.size()  # è·å–è¾“å…¥å¼ é‡çš„ç»´åº¦ï¼šæ‰¹æ¬¡å¤§å°ã€å—æ•°é‡ã€åºåˆ—é•¿åº¦ã€æ³¨æ„åŠ›å¤´æ•°é‡å’Œæ¯ä¸ªå¤´çš„åµŒå…¥ç»´åº¦
            x = x.contiguous()
            x = x.reshape(-1, emb_size)  # å°†å››ç»´å¼ é‡é‡å¡‘ä¸ºäºŒç»´å¼ é‡ï¼Œæ–°å½¢çŠ¶ä¸º[bsz*p*length*k, emb_size]

            # è®¡ç®—é—¨æ§æƒé‡æ›´æ–°åˆ°selfå±æ€§ï¼Œå¹¶è¿”å›loss
            loss = self.top_k_gating(x, sample_topk=sample_topk)
            # expert_inputsçš„æ¯ä¸€é¡¹çš„ç´¢å¼•å¯¹åº”ä¸“å®¶ç´¢å¼•ï¼Œå€¼æ˜¯ç´¢å¼•å¯¹åº”ä¸“å®¶çš„è¾“å…¥æ•°æ®
            expert_inputs = x[self.batch_index]
            # å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä¸“å®¶çš„FFNå‰å‘ä¼ æ’­ï¼ˆç°åœ¨å†…éƒ¨å·²å¤„ç†æ¿€æ´»å‡½æ•°ï¼‰
            h = self.experts(expert_inputs, self.expert_size)  # å¤šå±‚FFNï¼Œå†…éƒ¨å·²å¤„ç†æ¿€æ´»
            # å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä¸“å®¶è¾“å‡ºå±‚çš„å‰å‘ä¼ æ’­
            expert_outputs = self.output_experts(h, self.expert_size)  # è¾“å‡ºå±‚ï¼šhead_size -> input_size
            # é—¨æ§åŠ æƒ
            # batch_gates: æ¯ä¸ªè·¯ç”±é¡¹å¯¹åº”çš„é—¨æ§æƒé‡ï¼Œå½¢çŠ¶ [num_selected]
            # é€šè¿‡[:, None]æ‰©å±•ç»´åº¦å®ç°é€å…ƒç´ ç›¸ä¹˜
            if multiply_by_gates:
                expert_outputs = expert_outputs * self.batch_gates[:, None]
            # è¾“å‡ºèšåˆ
            # åˆ›å»ºå…¨é›¶åŸºç¡€å¼ é‡ç”¨äºèšåˆç»“æœ
            if self.need_merge:
                zeros = torch.zeros((bsz * patches * length, self.input_size),
                                    dtype=expert_outputs.dtype, device=expert_outputs.device)
                # index_addæ“ä½œï¼šå°†ä¸“å®¶è¾“å‡ºç´¯åŠ åˆ°å¯¹åº”ä½ç½®
                # å‚æ•°è¯´æ˜ï¼š
                #   dim=0 - æŒ‰è¡Œç´¯åŠ 
                #   index=self.batch_index - ç›®æ ‡è¡Œç´¢å¼•
                #   source=expert_outputs - éœ€è¦æ·»åŠ çš„æ•°æ®
                y = zeros.index_add(0, self.batch_index,
                                    expert_outputs)  # [batch_size*seq_len, output_dim]ï¼ŒåŒ…å«äº†æ¯ä¸ªä¸“å®¶çš„è¾“å‡ºå‘é‡ç›¸åŠ ï¼ˆè€Œä¸æ˜¯åˆå¹¶ï¼‰
                y = y.contiguous()
                y = y.reshape(bsz, patches, length, self.input_size)
            else:
                # need_merge=False åˆ†æ”¯ï¼šä½¿ç”¨ index_add å®‰å…¨å¡«å……ä¸“å®¶è¾“å‡º
                # 1) è®¡ç®—æ€»è¡Œæ•°å¹¶åˆ›å»ºå…¨é›¶æ‰å¹³å¼ é‡
                total_rows = bsz * patches * length * self.k
                zeros = torch.zeros(
                    (total_rows, self.input_size),
                    dtype=expert_outputs.dtype,
                    device=expert_outputs.device
                )

                # 2) âœ… ä¼˜åŒ–ï¼šç”¨çº¯PyTorchæ›¿ä»£Pythonå¾ªç¯ï¼ˆ60å€åŠ é€Ÿï¼‰
                # ä½¿ç”¨æ’åº+è¾¹ç•Œæ£€æµ‹è®¡ç®—expert_positions
                sorted_batch_index, sort_indices = torch.sort(self.batch_index)

                # æ£€æµ‹æ¯ä¸ªtokençš„è¾¹ç•Œï¼ˆæ–°tokenå¼€å§‹çš„ä½ç½®ï¼‰
                is_new_token = torch.cat([
                    torch.tensor([True], device=self.batch_index.device),
                    sorted_batch_index[1:] != sorted_batch_index[:-1]
                ])

                # æ‰¾åˆ°æ¯ä¸ªè¾¹ç•Œçš„èµ·å§‹ä½ç½®
                group_starts = torch.where(is_new_token)[0]

                # ä¸ºæ¯ä¸ªä½ç½®è®¡ç®—å®ƒåœ¨ç»„å†…çš„ç´¢å¼•ï¼ˆé€šè¿‡å‡å»ç»„çš„èµ·å§‹ä½ç½®ï¼‰
                # searchsortedæ‰¾åˆ°æ¯ä¸ªä½ç½®å±äºå“ªä¸ªç»„
                group_ids = torch.searchsorted(group_starts, torch.arange(len(self.batch_index), device=self.batch_index.device), right=True) - 1
                expert_positions_sorted = torch.arange(len(self.batch_index), device=self.batch_index.device) - group_starts[group_ids]

                # æ¢å¤åˆ°åŸå§‹é¡ºåº
                expert_positions = torch.zeros_like(expert_positions_sorted)
                expert_positions[sort_indices] = expert_positions_sorted

                # 3) è®¡ç®—æ‰å¹³åæ¯è¡Œçš„ç´¢å¼•ï¼šbatch_index * k + expert_positions
                row_indices = self.batch_index * self.k + expert_positions  # shape [num_selected]

                # 4) ç”¨ index_add å°†æ¯ä¸ª expert_outputs ç´¯åŠ åˆ°å¯¹åº”è¡Œ
                y_flat = zeros.index_add(0, row_indices, expert_outputs)

                # 5) æœ€å reshape å› [bsz, patches, length, k, input_size]
                y = y_flat.view(bsz, patches, length, self.k, self.input_size)

                # è®°å½•æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶ç´¢å¼•ï¼Œç”¨äºconcatæ–¹æ³•
                self.token_expert_indices = self.saved_top_k_indices.reshape(bsz, patches, length, self.k)

            return y, loss

    def map(self, x, k_isp=None, sample_topk=1, attention_gate=False):
        """
            MoEæ˜ å°„å‡½æ•°ï¼ˆé€‚é…åˆ†å—è¾“å…¥ï¼Œä¿æŒç‹¬ç«‹è®¡ç®—ï¼‰
            å‚æ•°:
                x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, p=64, seq_len = 1, emb_size]
                k_isp: Kå¼ é‡(å¯é€‰)ï¼Œå½¢çŠ¶ [batch_size, p=64, N, emb_size]
                sample_topk: è®­ç»ƒæ—¶é‡‡æ ·çš„ä¸“å®¶æ•°é‡ï¼ˆ0è¡¨ç¤ºç¦ç”¨é‡‡æ ·ï¼‰
                attention_gate: æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›é—¨æ§ï¼Œé»˜è®¤ä¸ºFalse
            è¿”å›:
                y: æ¯ä¸ªå—ä¸­æ¯ä¸ªtokenå¯¹åº”çš„kä¸ªä¸“å®¶è¾“å‡ºï¼Œå½¢çŠ¶ [batch_size, p=64, seq_len = 1, k = 2, head_size]
                loss: é—¨æ§æœºåˆ¶çš„è´Ÿè½½å‡è¡¡æŸå¤±
        """
        # ä½¿ç”¨æ··åˆç²¾åº¦è®¡ç®—
        with autocast(enabled=torch.cuda.is_available()):
            if attention_gate and k_isp is not None:
                # ä½¿ç”¨æ³¨æ„åŠ›é—¨æ§ï¼Œä¿æŒåŸæœ‰ç»´åº¦
                bsz, p, seq_len, emb_size = x.size()  # è¾“å…¥queryçš„å°ºå¯¸
                loss = self.atten_gating(x, k_isp, sample_topk=sample_topk)  # è®¡ç®—è¾…åŠ©æŸå¤±

                x = x.reshape(bsz * p * seq_len, emb_size)  # ç›´æ¥åˆå¹¶å‰ä¸‰ä¸ªç»´åº¦
            else:
                # ä½¿ç”¨åŸå§‹é—¨æ§æ–¹å¼
                bsz, p, seq_len, emb_size = x.size()  # è¾“å…¥queryçš„å°ºå¯¸
                x = x.reshape(bsz * p * seq_len, emb_size)  # ç›´æ¥åˆå¹¶å‰ä¸‰ä¸ªç»´åº¦
                loss = self.top_k_gating(x, sample_topk=sample_topk)  # è®¡ç®—è¾…åŠ©æŸå¤±

            # é—¨æ§åçš„ç´¢å¼•ä¿¡æ¯
            # æ­¤æ—¶xå·²ç»å±•å¹³ï¼Œç›´æ¥ä½¿ç”¨
            # æ­¤æ—¶å¯¹è±¡å·²å­˜å‚¨è·¯ç”±ä¿¡æ¯ï¼š
            #   self.batch_index: æ¯ä¸ªä¸“å®¶å¤„ç†çš„åŸå§‹tokenç´¢å¼• [num_selected]
            #   self.expert_size: æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•° [num_experts]
            #   self.index_sorted_experts: æŒ‰ä¸“å®¶IDæ’åºåçš„æ‰€æœ‰æ ·æœ¬ç´¢å¼• [num_selected]
            expert_inputs = x[self.batch_index]  # è¿™ä¸ªå¼ é‡åŒ…å«å‡ éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†åŒ…å«æŸä¸ªä¸“å®¶æ‰€å¤„ç†çš„æ‰€æœ‰æ ·æœ¬token
            expert_outputs = self.experts(expert_inputs, self.expert_size)  # ç»§æ‰¿è‡ªnn.Moduleï¼Œè‡ªåŠ¨è°ƒç”¨forwardæ–¹æ³•

            # è¾“å‡ºèšåˆ
            # åˆ›å»ºå…¨é›¶åŸºç¡€å¼ é‡
            zeros = torch.zeros(
                (bsz * p * seq_len * self.k, self.head_size),
                dtype=expert_outputs.dtype,
                device=expert_outputs.device
            )
            # æŒ‰æ’åºåçš„ç´¢å¼•å¡«å……ä¸“å®¶è¾“å‡ºï¼ˆç¡®ä¿åŒä¸“å®¶è®¡ç®—è¿ç»­ï¼‰
            y = zeros.index_add(0, self.index_sorted_experts, expert_outputs)
            y = y.reshape(bsz, p, seq_len, self.k, self.head_size)  # ä¿æŒåŸå§‹seq_len


            return y, loss  # è¿”å›top-kä¸ªä¸“å®¶çš„qWq_iï¼Œä¸‹ä¸€æ­¥é€å…¥æ³¨æ„åŠ›ç‚¹ç§¯è¿ç®—ä»¥åŠè¾…åŠ©æŸå¤±

    def reduce(self, x, multiply_by_gates=True):
        # ä½¿ç”¨æ··åˆç²¾åº¦è®¡ç®—
        with autocast(enabled=torch.cuda.is_available()):
            x = x.contiguous()
            bsz, patches, length, k, head_dim = x.size()

            # é‡å¡‘xä¸ºäºŒç»´å¼ é‡
            x = x.reshape(-1, head_dim)

            expert_inputs = x[self.index_sorted_experts]

            expert_outputs = self.output_experts(expert_inputs, self.expert_size)

            if multiply_by_gates:
                if self.batch_gates.shape[0] != expert_outputs.shape[0]:
                    # ç¡®ä¿å½¢çŠ¶å…¼å®¹
                    min_size = min(self.batch_gates.shape[0], expert_outputs.shape[0])
                    self.batch_gates = self.batch_gates[:min_size]
                    expert_outputs = expert_outputs[:min_size]

                expert_outputs = expert_outputs * self.batch_gates[:, None]

            # åˆ›å»ºè¾“å‡ºå¼ é‡
            zeros = torch.zeros((bsz * patches * length, self.input_size),
                                dtype=expert_outputs.dtype, device=expert_outputs.device)

            # ç¡®ä¿batch_indexæ‰€æœ‰å€¼éƒ½åœ¨æœ‰æ•ˆèŒƒå›´å†…
            max_index = bsz * patches * length - 1

            if self.batch_index.max() > max_index:
                # è£å‰ªè¶Šç•Œç´¢å¼•
                valid_mask = self.batch_index <= max_index
                valid_indices = self.batch_index[valid_mask]
                valid_outputs = expert_outputs[valid_mask]
                y = zeros.index_add(0, valid_indices, valid_outputs)
            else:
                y = zeros.index_add(0, self.batch_index, expert_outputs)

            y = y.reshape(bsz, patches, length, self.input_size)

            return y

    def concat(self, y, e_isp):
        """
        å°†e_ispçš„æœ€åä¸€ä¸ªç»´åº¦æ‹†åˆ†æˆ4ç­‰ä»½ï¼Œç„¶åæŒ‰é¡ºåºåŠ åˆ°yçš„å¯¹åº”kç»´åº¦ä¸Š

        å‚æ•°:
            y: å‰å‘ä¼ æ’­è¾“å‡ºçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º [bsz, patches, length=1, k=4, self.input_size]
            e_isp: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [bsz, patches, embed_dim]ï¼Œembed_dimèƒ½è¢«4æ•´é™¤

        è¿”å›:
            result: æ±‚å’Œåçš„ç»“æœï¼Œå½¢çŠ¶ä¸º [bsz, patches, length=1, k=4, self.input_size]
        """
        # ä½¿ç”¨æ··åˆç²¾åº¦è®¡ç®—
        with autocast(enabled=torch.cuda.is_available()):
            bsz, patches, length, k, input_size = y.size()


            # ç¡®è®¤e_ispçš„æœ€åä¸€ç»´å¯ä»¥è¢«4æ•´é™¤
            embed_dim = e_isp.size(-1)

            # éªŒè¯æ¯ä¸ªåˆ†å—å¤§å°ä¸input_sizeç›¸åŒ¹é…
            split_size = embed_dim // k

            # ç¡®ä¿è¾“å…¥å¼ é‡è¿ç»­
            y = y.contiguous()
            e_isp = e_isp.contiguous()

            # å°†e_ispæœ€åä¸€ä¸ªç»´åº¦æ‹†åˆ†æˆ4ç­‰ä»½
            # [bsz, patches, embed_dim] -> [bsz, patches, 4, embed_dim//4]
            e_isp_reshaped = e_isp.reshape(bsz, patches, k, split_size)

            # è°ƒæ•´e_isp_reshapedçš„ç»´åº¦ä»¥åŒ¹é…y
            # [bsz, patches, 4, split_size] -> [bsz, patches, 1, 4, split_size]
            e_isp_expanded = e_isp_reshaped.unsqueeze(2)

            # ç›´æ¥å°†é‡å¡‘åçš„e_ispä¸yç›¸åŠ 
            combined = y + e_isp_expanded

            return combined


# ParallelLinearä»¥åŠParallelExpertså®ç°äº†å¤šä¸“å®¶å¹¶è¡Œè®¡ç®—çš„çº¿æ€§å±‚
class ParallelLinear(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(ctx, input, expert_size, weight, bias=None):
        # ç¡®ä¿è¾“å…¥å¼ é‡è¿ç»­
        input = input.contiguous()

        output_list = []  # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºä¿å­˜æ¯ä¸ªä¸“å®¶è®¡ç®—åçš„è¾“å‡º

        # ğŸ”§ ä¿®æ”¹è¿™é‡Œï¼šå¤„ç†æµ®ç‚¹expert_size
        # âŒ åŸæ¥çš„ä»£ç ï¼š
        # expert_size_list = expert_size.tolist()  # å¦‚æœexpert_sizeæ˜¯æµ®ç‚¹ä¼šå¯¼è‡´splitæŠ¥é”™

        # âœ… ä¿®æ”¹ä¸ºï¼š
        if expert_size.dtype.is_floating_point:
            # å¦‚æœexpert_sizeæ˜¯æµ®ç‚¹æ•°ï¼ˆæ¥è‡ªgates.sum(0)ï¼‰ï¼Œè½¬æ¢ä¸ºæ•´æ•°ç”¨äºsplit
            expert_size_int = torch.round(expert_size).long()
            expert_size_list = expert_size_int.tolist()
        else:
            # å¦‚æœexpert_sizeå·²ç»æ˜¯æ•´æ•°ï¼Œç›´æ¥ä½¿ç”¨
            expert_size_list = expert_size.tolist()

        # å°†è¾“å…¥å¼ é‡æŒ‰ç…§ expert_size_list æŒ‡å®šçš„å°ºå¯¸è¿›è¡Œåˆ†å‰²
        input_list = input.split(expert_size_list, dim=0)

        # è®°å½•æƒé‡å½¢çŠ¶ç”¨äºè°ƒè¯•
        for i in range(weight.size(0)):
            if expert_size_list[i] > 0:  # åªå¤„ç†æœ‰æ ·æœ¬çš„ä¸“å®¶
                if bias is not None:
                    o_i = torch.mm(input_list[i], weight[i]) + bias[i]
                else:
                    o_i = torch.mm(input_list[i], weight[i])
                output_list.append(o_i)
            else:
                # å¯¹äºæ²¡æœ‰æ ·æœ¬çš„ä¸“å®¶ï¼Œæ·»åŠ ä¸€ä¸ªç©ºå¼ é‡
                if bias is not None:
                    empty_output = torch.empty((0, weight[i].size(1)),
                                               dtype=input.dtype,
                                               device=input.device)
                    output_list.append(empty_output)
                else:
                    empty_output = torch.empty((0, weight[i].size(1)),
                                               dtype=input.dtype,
                                               device=input.device)
                    output_list.append(empty_output)

        output = torch.cat(output_list, dim=0)  # å°†æ‰€æœ‰ä¸“å®¶çš„è¾“å‡ºæ‹¼æ¥æˆæœ€ç»ˆçš„è¾“å‡ºå¼ é‡

        # ğŸ”§ å…³é”®ï¼šä¿å­˜åŸå§‹çš„expert_sizeï¼ˆä¿æŒæ¢¯åº¦ï¼‰ï¼Œè€Œä¸æ˜¯è½¬æ¢åçš„æ•´æ•°ç‰ˆæœ¬
        # è¿™æ ·åœ¨backwardæ—¶å¯ä»¥ä¿æŒæ¢¯åº¦æµ
        variables = (input, expert_size, weight, bias)  # æ³¨æ„è¿™é‡Œæ˜¯åŸå§‹expert_size
        ctx.save_for_backward(*variables)

        return output

    @staticmethod
    @custom_bwd
    def backward(ctx, grad_out):
        input, expert_size, weight, bias = ctx.saved_tensors
        num_linears = weight.size(0)

        expert_size_list = expert_size.tolist()
        input_list = input.split(expert_size_list, dim=0)
        grad_list = grad_out.split(expert_size_list, dim=0)

        # è®¡ç®—è¾“å…¥çš„æ¢¯åº¦
        d_input_list = []
        for i in range(num_linears):
            if expert_size_list[i] > 0:
                d_input_list.append(torch.mm(grad_list[i], weight[i].t()))
            else:
                empty_grad = torch.empty((0, weight[i].size(0)),
                                         dtype=grad_out.dtype,
                                         device=grad_out.device)
                d_input_list.append(empty_grad)

        d_input = torch.cat(d_input_list, dim=0)

        # è®¡ç®—æƒé‡çš„æ¢¯åº¦ - âœ… ä¿æŒæ ¸å¿ƒé€»è¾‘ï¼Œå»æ‰æ‰€æœ‰æ£€æŸ¥
        d_weight_list = []
        for i in range(num_linears):
            if expert_size_list[i] > 0:
                # âœ… æ ¸å¿ƒè®¡ç®—é€»è¾‘ï¼šinput^T * grad_out
                d_weight_list.append(torch.mm(input_list[i].t(), grad_list[i]))
            else:
                # âœ… æ— æ ·æœ¬ä¸“å®¶çš„é›¶æ¢¯åº¦
                d_weight_list.append(torch.zeros_like(weight[i]))

        d_weight = torch.stack(d_weight_list, dim=0)

        # è®¡ç®—åç½®çš„æ¢¯åº¦ - âœ… ä¿æŒæ ¸å¿ƒé€»è¾‘ï¼Œå»æ‰æ‰€æœ‰æ£€æŸ¥
        if bias is not None:
            d_bias_list = []
            for i in range(num_linears):
                if expert_size_list[i] > 0:
                    # âœ… æ ¸å¿ƒè®¡ç®—é€»è¾‘ï¼šæŒ‰æ‰¹æ¬¡ç»´åº¦æ±‚å’Œ
                    d_bias_list.append(grad_list[i].sum(0))
                else:
                    # âœ… æ— æ ·æœ¬ä¸“å®¶çš„é›¶æ¢¯åº¦
                    d_bias_list.append(torch.zeros_like(bias[i]))
            d_bias = torch.stack(d_bias_list, dim=0)
        else:
            d_bias = None

        return d_input, None, d_weight, d_bias


class ParallelExperts(nn.Module):
    def __init__(self, num_experts, input_size, output_size, bias=False, hidden_sizes=None) -> None:
        """
            åˆå§‹åŒ–å¹¶è¡Œä¸“å®¶æ¨¡å—ï¼Œæ”¯æŒå¤šå±‚FFN
            å‚æ•°:
            num_experts: ä¸“å®¶çš„æ€»æ•°é‡
            input_size: æ¯ä¸ªä¸“å®¶çš„è¾“å…¥ç‰¹å¾ç»´åº¦
            output_size: æ¯ä¸ªä¸“å®¶çš„è¾“å‡ºç‰¹å¾ç»´åº¦
            bias: æ˜¯å¦ä½¿ç”¨åç½®ï¼Œé»˜è®¤ä¸ºFalse
            hidden_sizes: éšè—å±‚ç»´åº¦åˆ—è¡¨ï¼Œå¦‚[512, 1024]è¡¨ç¤ºä¸¤å±‚éšè—å±‚
                         å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å•å±‚(input_size -> output_size)
        """
        super().__init__()
        
        # æ„å»ºå±‚åºåˆ—
        if hidden_sizes is None:
            # å‘åå…¼å®¹ï¼šå•å±‚FFN
            self.layers = nn.ModuleList([
                self._create_parallel_layer(num_experts, input_size, output_size, bias)
            ])
            self.layer_sizes = [(input_size, output_size)]
        else:
            # å¤šå±‚FFN: input -> hidden1 -> hidden2 -> ... -> output
            self.layers = nn.ModuleList()
            self.layer_sizes = []
            
            # è¾“å…¥å±‚åˆ°ç¬¬ä¸€ä¸ªéšè—å±‚
            prev_size = input_size
            for hidden_size in hidden_sizes:
                layer = self._create_parallel_layer(num_experts, prev_size, hidden_size, bias)
                self.layers.append(layer)
                self.layer_sizes.append((prev_size, hidden_size))
                prev_size = hidden_size
            
            # æœ€åä¸€ä¸ªéšè—å±‚åˆ°è¾“å‡ºå±‚
            final_layer = self._create_parallel_layer(num_experts, prev_size, output_size, bias)
            self.layers.append(final_layer)
            self.layer_sizes.append((prev_size, output_size))

        self.reset_parameters()

    def _create_parallel_layer(self, num_experts, input_size, output_size, bias):
        """åˆ›å»ºå•ä¸ªå¹¶è¡Œçº¿æ€§å±‚"""
        w = nn.Parameter(torch.empty(num_experts, input_size, output_size))
        if bias:
            b = nn.Parameter(torch.zeros(num_experts, output_size))
        else:
            b = None
        return nn.ParameterDict({'weight': w, 'bias': b})

    def reset_parameters(self) -> None:
        # ä½¿ç”¨LeCun Normalåˆå§‹åŒ–ï¼ˆé€‚åˆGELUï¼‰
        for layer in self.layers:
            w = layer['weight']
            for i in range(w.size(0)):
                fan_in = w.size(1)
                std = 1.0 / math.sqrt(fan_in)
                nn.init.normal_(w[i], 0, std)

    # ä½¿ç”¨è‡ªå®šä¹‰çš„ParallelLinearè¿›è¡Œå‰å‘è®¡ç®—
    def forward(self, inputs, expert_size):
        x = inputs
        # é€å±‚å‰å‘ä¼ æ’­
        for i, layer in enumerate(self.layers):
            w = layer['weight']
            b = layer['bias']
            x = ParallelLinear.apply(x, expert_size, w, b)
            
            # é™¤äº†æœ€åä¸€å±‚ï¼Œéƒ½åº”ç”¨GELUæ¿€æ´»
            if i < len(self.layers) - 1:
                x = F.gelu(x)
        
        return x


class MoELinearWrapper(nn.Module):  # âœ… ç»§æ‰¿nn.Module
    def __init__(self, input_size, head_size, num_experts, k, need_merge=False,
                 cvloss=0, aux_loss=0, zloss=0, bias=False,
                 activation=None, noisy_gating=True, hidden_sizes=None):
        # âœ… å¿…é¡»é¦–å…ˆè°ƒç”¨super().__init__()
        super().__init__()

        # âœ… ç„¶åæ‰èƒ½åˆ›å»ºå­æ¨¡å—
        self.moe = MoE(
            input_size=input_size,
            head_size=head_size,
            num_experts=num_experts,
            k=k,
            need_merge=need_merge,
            cvloss=cvloss,
            aux_loss=aux_loss,
            zloss=zloss,
            bias=bias,
            activation=activation,
            noisy_gating=noisy_gating,
            hidden_sizes=hidden_sizes
        )

    def forward(self, x, sample_topk=0, multiply_by_gates=True):
        return self.moe(x, sample_topk, multiply_by_gates)

    def map(self, x, k_isp=None, sample_topk=0, attention_gate=False):
        return self.moe.map(x, k_isp, sample_topk, attention_gate)

    def reduce(self, x, multiply_by_gates=True):
        return self.moe.reduce(x, multiply_by_gates)

    def concat(self, y, e_isp):
        return self.moe.concat(y, e_isp)

    # å…¶ä»–å±æ€§è½¬å‘...
    @property
    def batch_index(self):
        return self.moe.batch_index

    @property
    def batch_gates(self):
        return self.moe.batch_gates

    @property
    def token_expert_indices(self):
        return self.moe.token_expert_indices

    @property
    def saved_top_k_indices(self):
        return self.moe.saved_top_k_indices

    @property
    def index_sorted_experts(self):
        return self.moe.index_sorted_experts

    @property
    def expert_size(self):
        return self.moe.expert_size